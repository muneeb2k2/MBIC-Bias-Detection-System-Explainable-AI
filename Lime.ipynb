{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "News Bias Classification System - Open Source Implementation\n",
        "Two-Stage Hybrid Architecture: Hugging Face + Llama 3\n",
        "\n",
        "Stage 1: Binary screening (Local BERT model - FREE)\n",
        "Stage 2: Subtype reasoning (Llama 3 via HF API - Cost-effective)\n",
        "\n",
        "Author: Migrated from OpenAI to open-source solution\n",
        "Dataset: MBIC (Media Bias Annotation by Crowdsourcing and Experts)\n",
        "Goal: Multi-label bias classification with explainability\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "X4qlTtVRnYQc",
        "outputId": "794fdb09-d492-4ccf-e2af-711533a623a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNews Bias Classification System - Open Source Implementation\\nTwo-Stage Hybrid Architecture: Hugging Face + Llama 3\\n\\nStage 1: Binary screening (Local BERT model - FREE)\\nStage 2: Subtype reasoning (Llama 3 via HF API - Cost-effective)\\n\\nAuthor: Migrated from OpenAI to open-source solution\\nDataset: MBIC (Media Bias Annotation by Crowdsourcing and Experts)\\nGoal: Multi-label bias classification with explainability\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import requests\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass, field\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (f1_score, precision_score, recall_score,\n",
        "                            classification_report, confusion_matrix)\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "tzfcBoTEngwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def setup_colab_environment():\n",
        "    print(\"DEBUG: Function started\")  # <-- Does this show?\n",
        "\n",
        "    try:\n",
        "        import google.colab\n",
        "        print(\"DEBUG: google.colab imported\")  # test\n",
        "        IN_COLAB = True\n",
        "        print(\"ðŸ”µ Running in Google Colab\")\n",
        "\n",
        "        print(\"\\nðŸ“¦ Installing required packages...\")\n",
        "        os.system('pip install -q transformers torch accelerate requests lime openpyxl')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"ERROR inside function:\", e)   # show errors clearly\n",
        "        IN_COLAB = False\n",
        "        print(\"ðŸ”µ Running in local environment\")\n",
        "\n",
        "    return IN_COLAB\n",
        "\n",
        "setup_colab_environment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfRoXfDdnjSZ",
        "outputId": "63aed2a4-9bf8-4e95-c60f-70384d3c3936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Function started\n",
            "DEBUG: google.colab imported\n",
            "ðŸ”µ Running in Google Colab\n",
            "\n",
            "ðŸ“¦ Installing required packages...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PROJECT CONFIGURATION (UPDATED)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "@dataclass\n",
        "class ProjectConfig:\n",
        "    \"\"\"\n",
        "    Centralized configuration for the MBIC Bias Classification pipeline.\n",
        "    Includes dataset split ratios, evaluation thresholds, API settings, and\n",
        "    secure Hugging Face token handling.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- API Token (SAFE: pulled from environment variable) ---\n",
        "    HUGGINGFACE_TOKEN: str = field(default_factory=lambda: os.getenv(\"HF_TOKEN\", None))\n",
        "\n",
        "    # --- Two-Stage Model Architecture ---\n",
        "    BINARY_MODEL: str = \"newsmediabias/UnBIAS-classifier\"           # Stage 1 (local)\n",
        "    SUBTYPE_MODEL: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"      # Stage 2 (API)\n",
        "    INFERENCE_API_URL: str = \"https://api-inference.huggingface.co/models/\"\n",
        "\n",
        "    # --- Dataset + Output Paths ---\n",
        "    MBIC_FILEPATH: str = \"/content/Copy of labeled_dataset.xlsx\"\n",
        "    OUTPUT_DIR: str = \"bias_classification_outputs\"\n",
        "\n",
        "    # --- Bias Subtypes ---\n",
        "    BIAS_SUBTYPES: List[str] = field(default_factory=lambda: [\n",
        "        \"Political Bias\",\n",
        "        \"Linguistic Bias\",\n",
        "        \"Racial Bias\",\n",
        "        \"Gender Bias\",\n",
        "        \"Text-level Context\",\n",
        "        \"Reporting-level Context\",\n",
        "        \"Cognitive Bias\",\n",
        "        \"Fake News\",\n",
        "        \"Hate Speech\"\n",
        "    ])\n",
        "\n",
        "    # --- Dataset Split Ratios ---\n",
        "    TRAIN_RATIO: float = 0.70\n",
        "    VAL_RATIO: float = 0.10\n",
        "    TEST_RATIO: float = 0.20\n",
        "\n",
        "    # --- Evaluation Settings ---\n",
        "    HUMAN_VALIDATION_SAMPLE_SIZE: int = 100\n",
        "    F1_THRESHOLD: float = 0.80\n",
        "\n",
        "    # --- API Settings ---\n",
        "    MAX_API_RETRIES: int = 3\n",
        "    API_TIMEOUT: int = 30\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Create output directory and validate token & split ratios.\"\"\"\n",
        "        # Create output directory if missing\n",
        "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        # Hugging Face token check\n",
        "        if not self.HUGGINGFACE_TOKEN:\n",
        "            print(\"âš ï¸ WARNING: No Hugging Face token found. Set HF_TOKEN environment variable or use set_hf_token()\")\n",
        "        else:\n",
        "            print(\"ðŸ”‘ Hugging Face token loaded successfully.\")\n",
        "\n",
        "        # Validate dataset split ratios sum to 1.0\n",
        "        total_ratio = self.TRAIN_RATIO + self.VAL_RATIO + self.TEST_RATIO\n",
        "        if not abs(total_ratio - 1.0) < 1e-6:\n",
        "            print(f\"âš ï¸ Warning: Dataset split ratios sum to {total_ratio}, not 1.0\")\n",
        "\n",
        "    def set_hf_token(self, token: str):\n",
        "        \"\"\"Set or update Hugging Face API token\"\"\"\n",
        "        self.HUGGINGFACE_TOKEN = token\n",
        "        os.environ['HF_TOKEN'] = token\n",
        "        print(\"ðŸ”‘ Hugging Face token updated successfully.\")\n"
      ],
      "metadata": {
        "id": "-9c3qyN8oRr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8vD_QWAmpH2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "    # Dataset split ratios\n",
        "    TRAIN_RATIO: float = 0.70\n",
        "    VAL_RATIO: float = 0.10\n",
        "    TEST_RATIO: float = 0.20\n",
        "\n",
        "    # Evaluation settings\n",
        "    HUMAN_VALIDATION_SAMPLE_SIZE: int = 100\n",
        "    F1_THRESHOLD: float = 0.80\n",
        "\n",
        "    # API settings\n",
        "    MAX_API_RETRIES: int = 3\n",
        "    API_TIMEOUT: int = 30\n",
        "\n",
        "    def __post_init__(self):\n",
        "        os.makedirs(self.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    def set_hf_token(self, token: str):\n",
        "        \"\"\"Set Hugging Face API token\"\"\"\n",
        "        self.HUGGINGFACE_TOKEN = token\n",
        "        os.environ['HUGGINGFACE_TOKEN'] = token\n",
        "\n",
        "\n",
        "# Comprehensive bias definitions for LLM prompting\n",
        "BIAS_DEFINITIONS = {\n",
        "    \"Political Bias\": \"\"\"\n",
        "    Favoring or opposing specific political parties, ideologies, or figures.\n",
        "    Examples: Partisan language, selective coverage of political events,\n",
        "    framing issues to benefit one political side.\n",
        "    \"\"\",\n",
        "\n",
        "    \"Linguistic Bias\": \"\"\"\n",
        "    Word choice that influences perception through loaded language, euphemisms,\n",
        "    or emotionally charged terms. Examples: \"freedom fighter\" vs \"terrorist\",\n",
        "    \"tax relief\" vs \"tax cuts\", \"illegal aliens\" vs \"undocumented immigrants\".\n",
        "    \"\"\",\n",
        "\n",
        "    \"Racial Bias\": \"\"\"\n",
        "    Stereotypes, prejudice, or discrimination based on race or ethnicity.\n",
        "    Examples: Racial profiling, stereotypical associations,\n",
        "    disproportionate negative coverage of specific racial groups.\n",
        "    \"\"\",\n",
        "\n",
        "    \"Gender Bias\": \"\"\"\n",
        "    Stereotypes or discrimination based on gender, including sexist language\n",
        "    or gendered expectations. Examples: \"Bossy\" for women vs \"assertive\" for men,\n",
        "    focus on appearance for women but competence for men.\n",
        "    \"\"\",\n",
        "\n",
        "    \"Text-level Context\": \"\"\"\n",
        "    Misleading framing or manipulation of context within the text itself.\n",
        "    Examples: Cherry-picking quotes, juxtaposing unrelated facts to create\n",
        "    false implications, misleading headlines.\n",
        "    \"\"\",\n",
        "\n",
        "    \"Reporting-level Context\": \"\"\"\n",
        "    Selective reporting, omission of important facts, or one-sided coverage.\n",
        "    Examples: Covering only negative aspects of a topic, ignoring counterarguments,\n",
        "    unbalanced source selection.\n",
        "    \"\"\",\n",
        "\n",
        "    \"Cognitive Bias\": \"\"\"\n",
        "    Logical fallacies or manipulative reasoning patterns that exploit\n",
        "    psychological biases. Examples: False dilemmas, slippery slope arguments,\n",
        "    appeal to emotion, confirmation bias reinforcement.\n",
        "    \"\"\",\n",
        "\n",
        "    \"Fake News\": \"\"\"\n",
        "    Demonstrably false information presented as fact, fabricated content,\n",
        "    or deliberately misleading claims. Examples: Made-up statistics,\n",
        "    false attributions, doctored quotes, conspiracy theories.\n",
        "    \"\"\",\n",
        "\n",
        "    \"Hate Speech\": \"\"\"\n",
        "    Hostile, threatening, or discriminatory language targeting individuals\n",
        "    or groups based on protected characteristics. Examples: Slurs,\n",
        "    dehumanizing language, incitement to violence or discrimination.\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE I: DATA LOADING & PREPARATION\n",
        "# ============================================================================\n",
        "\n",
        "class MBICDataLoader:\n",
        "    \"\"\"Load and prepare MBIC dataset - handles Excel, CSV, and JSON\"\"\"\n",
        "\n",
        "    def __init__(self, filepath: str):\n",
        "        self.filepath = filepath\n",
        "        self.df = None\n",
        "        self.df_filtered = None\n",
        "\n",
        "    def load_data(self) -> pd.DataFrame:\n",
        "        \"\"\"Load MBIC dataset - handles Excel, CSV, and JSON formats\"\"\"\n",
        "        file_ext = os.path.splitext(self.filepath)[1].lower()\n",
        "\n",
        "        try:\n",
        "            if file_ext in ['.xlsx', '.xls']:\n",
        "                # Load Excel file\n",
        "                print(f\"ðŸ“‚ Loading Excel file: {self.filepath}\")\n",
        "                self.df = pd.read_excel(self.filepath)\n",
        "                print(f\"âœ“ Loaded Excel: {len(self.df)} records\")\n",
        "\n",
        "            elif file_ext == '.csv':\n",
        "                # Load CSV file\n",
        "                print(f\"ðŸ“‚ Loading CSV file: {self.filepath}\")\n",
        "                self.df = pd.read_csv(self.filepath)\n",
        "                print(f\"âœ“ Loaded CSV: {len(self.df)} records\")\n",
        "\n",
        "            elif file_ext == '.json':\n",
        "                # Load JSON file\n",
        "                print(f\"ðŸ“‚ Loading JSON file: {self.filepath}\")\n",
        "                self.df = pd.read_json(self.filepath, lines=True)\n",
        "                print(f\"âœ“ Loaded JSON: {len(self.df)} records\")\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported file format: {file_ext}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Could not load dataset: {e}\")\n",
        "\n",
        "        print(f\"  Columns: {list(self.df.columns)}\")\n",
        "        self._standardize_columns()\n",
        "        return self.df\n",
        "\n",
        "    def _standardize_columns(self):\n",
        "        \"\"\"Standardize column names across different formats\"\"\"\n",
        "        column_mapping = {\n",
        "            'sentence': 'Sentence_Text',\n",
        "            'text': 'Sentence_Text',\n",
        "            'sentence_text': 'Sentence_Text',\n",
        "            'content': 'Sentence_Text',\n",
        "            'article': 'Sentence_Text',\n",
        "            'label': 'Binary_Label',\n",
        "            'bias_label': 'Binary_Label',\n",
        "            'bias': 'Binary_Label',\n",
        "            'classification': 'Binary_Label',\n",
        "            'id': 'sentence_id',\n",
        "            'sent_id': 'sentence_id',\n",
        "            'article_id': 'sentence_id'\n",
        "        }\n",
        "\n",
        "        for old_col, new_col in column_mapping.items():\n",
        "            if old_col in self.df.columns and new_col not in self.df.columns:\n",
        "                self.df.rename(columns={old_col: new_col}, inplace=True)\n",
        "\n",
        "        if 'sentence_id' not in self.df.columns:\n",
        "            self.df['sentence_id'] = [f\"sent_{i:04d}\" for i in range(len(self.df))]\n",
        "\n",
        "        # Handle Binary_Label if missing\n",
        "        if 'Binary_Label' not in self.df.columns:\n",
        "            print(\"âš ï¸  Warning: No Binary_Label column found. All sentences will be processed.\")\n",
        "            self.df['Binary_Label'] = 'Unknown'\n",
        "\n",
        "    def filter_no_agreement(self) -> pd.DataFrame:\n",
        "        \"\"\"Filter out 'No Agreement' cases\"\"\"\n",
        "        if 'Binary_Label' not in self.df.columns:\n",
        "            print(\"âš ï¸  No Binary_Label column - keeping all records\")\n",
        "            self.df_filtered = self.df.copy()\n",
        "        else:\n",
        "            self.df_filtered = self.df[\n",
        "                self.df['Binary_Label'] != 'No Agreement'\n",
        "            ].copy()\n",
        "\n",
        "        print(f\"\\n=== Phase I.1: MBIC Preparation ===\")\n",
        "        print(f\"Original size: {len(self.df)}\")\n",
        "        print(f\"After filtering 'No Agreement': {len(self.df_filtered)}\")\n",
        "\n",
        "        return self.df_filtered\n",
        "\n",
        "    def prepare_for_annotation(self) -> pd.DataFrame:\n",
        "        \"\"\"Full pipeline: load â†’ filter\"\"\"\n",
        "        self.load_data()\n",
        "        self.filter_no_agreement()\n",
        "        return self.df_filtered\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE I.2: SILVER LABEL GENERATION (TWO-STAGE ARCHITECTURE)\n",
        "# ============================================================================\n",
        "\n",
        "class SilverLabelAnnotator:\n",
        "    \"\"\"\n",
        "    Two-Stage Hybrid Annotator:\n",
        "    Stage 1: Binary classification (Local BERT - FREE)\n",
        "    Stage 2: Subtype reasoning (Llama 3 API - Only for biased)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: ProjectConfig):\n",
        "        self.config = config\n",
        "        self.bias_types = config.BIAS_SUBTYPES\n",
        "        self.annotation_log = []\n",
        "\n",
        "        # Stage 1: Initialize local binary classifier\n",
        "        print(\"\\nðŸ”„ Loading Stage 1: Binary Classifier (Local)...\")\n",
        "        self._initialize_binary_classifier()\n",
        "\n",
        "        # Stage 2: Setup API headers for Llama 3\n",
        "        self.api_headers = {\n",
        "            \"Authorization\": f\"Bearer {config.HUGGINGFACE_TOKEN}\"\n",
        "        }\n",
        "        self.api_url = f\"{config.INFERENCE_API_URL}{config.SUBTYPE_MODEL}\"\n",
        "\n",
        "        print(f\"âœ… Two-stage annotator ready\")\n",
        "        print(f\"   Stage 1: {config.BINARY_MODEL}\")\n",
        "        print(f\"   Stage 2: {config.SUBTYPE_MODEL}\")\n",
        "\n",
        "    def _initialize_binary_classifier(self):\n",
        "        \"\"\"Load local binary classifier (no API needed)\"\"\"\n",
        "        try:\n",
        "            from transformers import pipeline\n",
        "\n",
        "            # Load the UnBIAS binary classifier\n",
        "            self.binary_classifier = pipeline(\n",
        "                \"text-classification\",\n",
        "                model=self.config.BINARY_MODEL,\n",
        "                device=-1  # CPU (use 0 for GPU)\n",
        "            )\n",
        "            print(f\"   âœ“ Loaded: {self.config.BINARY_MODEL}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load binary classifier: {e}\")\n",
        "\n",
        "    def _stage1_binary_check(self, sentence: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Stage 1: Fast local binary classification\n",
        "        Returns: {'is_biased': bool, 'confidence': float}\n",
        "        \"\"\"\n",
        "        try:\n",
        "            result = self.binary_classifier(sentence, truncation=True, max_length=512)[0]\n",
        "\n",
        "            # Map model output to binary decision\n",
        "            label = result['label'].lower()\n",
        "            is_biased = 'bias' in label or label == 'biased'\n",
        "\n",
        "            return {\n",
        "                'is_biased': is_biased,\n",
        "                'confidence': float(result['score']),\n",
        "                'raw_label': result['label']\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Default to biased if error (to ensure Stage 2 review)\n",
        "            return {\n",
        "                'is_biased': True,\n",
        "                'confidence': 0.5,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    def _create_llama3_prompt(self, sentence: str, sentence_id: str) -> str:\n",
        "        \"\"\"Create prompt for Llama 3 subtype reasoning\"\"\"\n",
        "        definitions_text = \"\\n\\n\".join([\n",
        "            f\"{i+1}. **{bias_type}**\\n{BIAS_DEFINITIONS[bias_type].strip()}\"\n",
        "            for i, bias_type in enumerate(self.bias_types)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an expert media bias annotator. Your task is to classify news sentences into specific bias categories.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "========== BIAS TYPE DEFINITIONS ==========\n",
        "\n",
        "{definitions_text}\n",
        "\n",
        "========== CLASSIFICATION TASK ==========\n",
        "\n",
        "Sentence ID: {sentence_id}\n",
        "Sentence: \"{sentence}\"\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. This sentence has already been flagged as containing bias\n",
        "2. Identify which of the 9 bias types is MOST PROMINENT\n",
        "3. Provide a confidence score (0.0 to 1.0)\n",
        "4. Briefly explain the bias\n",
        "\n",
        "Return ONLY valid JSON with this exact structure:\n",
        "{{\n",
        "  \"bias_type\": \"<one of the 9 types above>\",\n",
        "  \"confidence_score\": 0.85,\n",
        "  \"bias_description\": \"<brief explanation>\"\n",
        "}}\n",
        "\n",
        "CRITICAL: bias_type must be EXACTLY one of: {', '.join(self.bias_types)}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "        return prompt\n",
        "\n",
        "    def _stage2_subtype_reasoning(self, sentence: str, sentence_id: str,\n",
        "                                  max_retries: int = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Stage 2: Call Llama 3 via Hugging Face Inference API\n",
        "        Returns: {'bias_type': str, 'confidence_score': float, 'bias_description': str}\n",
        "        \"\"\"\n",
        "        if max_retries is None:\n",
        "            max_retries = self.config.MAX_API_RETRIES\n",
        "\n",
        "        prompt = self._create_llama3_prompt(sentence, sentence_id)\n",
        "\n",
        "        payload = {\n",
        "            \"inputs\": prompt,\n",
        "            \"parameters\": {\n",
        "                \"max_new_tokens\": 200,\n",
        "                \"temperature\": 0.3,\n",
        "                \"top_p\": 0.9,\n",
        "                \"return_full_text\": False\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = requests.post(\n",
        "                    self.api_url,\n",
        "                    headers=self.api_headers,\n",
        "                    json=payload,\n",
        "                    timeout=self.config.API_TIMEOUT\n",
        "                )\n",
        "\n",
        "                if response.status_code == 503:\n",
        "                    # Model is loading, wait and retry\n",
        "                    wait_time = 2 ** attempt\n",
        "                    print(f\"   â³ Model loading, waiting {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Parse response\n",
        "                result = response.json()\n",
        "\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    content = result[0].get('generated_text', '')\n",
        "                else:\n",
        "                    content = result.get('generated_text', '')\n",
        "\n",
        "                # Extract JSON from response\n",
        "                content = content.strip()\n",
        "                if \"```json\" in content:\n",
        "                    content = content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "                elif \"```\" in content:\n",
        "                    content = content.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "                # Find JSON object\n",
        "                if '{' in content:\n",
        "                    start = content.index('{')\n",
        "                    end = content.rindex('}') + 1\n",
        "                    content = content[start:end]\n",
        "\n",
        "                parsed = json.loads(content)\n",
        "\n",
        "                # Validation\n",
        "                valid_types = self.bias_types\n",
        "                if parsed.get('bias_type') not in valid_types:\n",
        "                    # Try to match partial\n",
        "                    for valid_type in valid_types:\n",
        "                        if valid_type.lower() in parsed.get('bias_type', '').lower():\n",
        "                            parsed['bias_type'] = valid_type\n",
        "                            break\n",
        "                    else:\n",
        "                        parsed['bias_type'] = self.bias_types[0]  # Default\n",
        "\n",
        "                confidence = parsed.get('confidence_score', 0.7)\n",
        "                if not (0.0 <= confidence <= 1.0):\n",
        "                    confidence = 0.7\n",
        "                parsed['confidence_score'] = confidence\n",
        "\n",
        "                return {\n",
        "                    'bias_type': parsed['bias_type'],\n",
        "                    'confidence_score': confidence,\n",
        "                    'bias_description': parsed.get('bias_description', '')\n",
        "                }\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                if attempt == max_retries - 1:\n",
        "                    return {\n",
        "                        'bias_type': 'ERROR',\n",
        "                        'confidence_score': 0.0,\n",
        "                        'bias_description': 'API timeout'\n",
        "                    }\n",
        "                time.sleep(2 ** attempt)\n",
        "\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    return {\n",
        "                        'bias_type': 'ERROR',\n",
        "                        'confidence_score': 0.0,\n",
        "                        'bias_description': f'Error: {str(e)}'\n",
        "                    }\n",
        "                time.sleep(2 ** attempt)\n",
        "\n",
        "        return {\n",
        "            'bias_type': 'ERROR',\n",
        "            'confidence_score': 0.0,\n",
        "            'bias_description': 'Max retries exceeded'\n",
        "        }\n",
        "\n",
        "    def annotate_single_sentence(self, sentence: str, sentence_id: str) -> Dict:\n",
        "        \"\"\"\n",
        "        TWO-STAGE ANNOTATION PIPELINE\n",
        "        Stage 1: Binary check (local, fast, free)\n",
        "        Stage 2: Subtype reasoning (API, only if biased)\n",
        "        \"\"\"\n",
        "        # STAGE 1: Binary classification (local)\n",
        "        stage1_result = self._stage1_binary_check(sentence)\n",
        "\n",
        "        # If neutral, skip Stage 2 entirely\n",
        "        if not stage1_result['is_biased']:\n",
        "            self.annotation_log.append({\n",
        "                'sentence_id': sentence_id,\n",
        "                'status': 'neutral_skipped',\n",
        "                'stage1_confidence': stage1_result['confidence']\n",
        "            })\n",
        "\n",
        "            return {\n",
        "                'sentence_id': sentence_id,\n",
        "                'bias_type': 'None',\n",
        "                'confidence_score': stage1_result['confidence'],\n",
        "                'bias_description': 'Neutral/objective text',\n",
        "                'stage1_decision': 'neutral'\n",
        "            }\n",
        "\n",
        "        # STAGE 2: Subtype reasoning (API, only for biased)\n",
        "        stage2_result = self._stage2_subtype_reasoning(sentence, sentence_id)\n",
        "\n",
        "        self.annotation_log.append({\n",
        "            'sentence_id': sentence_id,\n",
        "            'status': 'biased_annotated',\n",
        "            'stage1_confidence': stage1_result['confidence'],\n",
        "            'stage2_type': stage2_result['bias_type']\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'sentence_id': sentence_id,\n",
        "            'bias_type': stage2_result['bias_type'],\n",
        "            'confidence_score': stage2_result['confidence_score'],\n",
        "            'bias_description': stage2_result['bias_description'],\n",
        "            'stage1_decision': 'biased',\n",
        "            'stage1_confidence': stage1_result['confidence']\n",
        "        }\n",
        "\n",
        "    def annotate_dataset(self, df: pd.DataFrame,\n",
        "                        sentence_col: str = 'Sentence_Text',\n",
        "                        id_col: str = 'sentence_id',\n",
        "                        save_checkpoints: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Annotate entire dataset with two-stage pipeline\n",
        "        \"\"\"\n",
        "        print(f\"\\n=== Phase I.2: Two-Stage Silver Label Generation ===\")\n",
        "        print(f\"ðŸŽ¯ Processing {len(df)} sentences\")\n",
        "        print(f\"ðŸ“Š Stage 1: {self.config.BINARY_MODEL} (Local)\")\n",
        "        print(f\"ðŸ“Š Stage 2: {self.config.SUBTYPE_MODEL} (API)\")\n",
        "\n",
        "        # Estimate biased sentences if Binary_Label exists\n",
        "        if 'Binary_Label' in df.columns and df['Binary_Label'].notna().any():\n",
        "            biased_estimate = len(df[df['Binary_Label'] == 'Biased'])\n",
        "            print(f\"â±ï¸  Estimated API calls: ~{biased_estimate}\\n\")\n",
        "        else:\n",
        "            print(f\"â±ï¸  Estimated API calls: TBD (will filter with Stage 1)\\n\")\n",
        "\n",
        "        results = []\n",
        "        checkpoint_interval = 50\n",
        "\n",
        "        stage1_count = 0\n",
        "        stage2_count = 0\n",
        "\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df),\n",
        "                            desc=\"Two-Stage Annotation\"):\n",
        "            sentence = str(row[sentence_col])\n",
        "            sent_id = str(row[id_col])\n",
        "\n",
        "            result = self.annotate_single_sentence(sentence, sent_id)\n",
        "            results.append(result)\n",
        "\n",
        "            # Track stage usage\n",
        "            if result['stage1_decision'] == 'neutral':\n",
        "                stage1_count += 1\n",
        "            else:\n",
        "                stage2_count += 1\n",
        "\n",
        "            # Checkpoint saving\n",
        "            if save_checkpoints and (idx + 1) % checkpoint_interval == 0:\n",
        "                checkpoint_df = pd.DataFrame(results)\n",
        "                checkpoint_path = os.path.join(\n",
        "                    self.config.OUTPUT_DIR,\n",
        "                    f'checkpoint_silver_{idx+1}.csv'\n",
        "                )\n",
        "                checkpoint_df.to_csv(checkpoint_path, index=False)\n",
        "                print(f\"\\nðŸ’¾ Checkpoint: {checkpoint_path}\")\n",
        "                print(f\"   Stage 1 only: {stage1_count}, Stage 2 calls: {stage2_count}\")\n",
        "\n",
        "            # Small delay only for API calls\n",
        "            if result['stage1_decision'] == 'biased':\n",
        "                time.sleep(0.5)\n",
        "\n",
        "        # Create results DataFrame\n",
        "        annotations_df = pd.DataFrame(results)\n",
        "\n",
        "        # Merge with original data\n",
        "        df_annotated = df.merge(\n",
        "            annotations_df,\n",
        "            left_on=id_col,\n",
        "            right_on='sentence_id',\n",
        "            how='left',\n",
        "            suffixes=('', '_annotation')\n",
        "        )\n",
        "\n",
        "        df_annotated['Subtype_Silver_Label'] = df_annotated['bias_type']\n",
        "        df_annotated['Silver_Confidence'] = df_annotated['confidence_score']\n",
        "\n",
        "        # Save final annotations\n",
        "        final_path = os.path.join(self.config.OUTPUT_DIR, 'silver_labels.csv')\n",
        "        df_annotated.to_csv(final_path, index=False)\n",
        "\n",
        "        self._print_summary(df_annotated, stage1_count, stage2_count)\n",
        "\n",
        "        return df_annotated\n",
        "\n",
        "    def _print_summary(self, df: pd.DataFrame, stage1_count: int, stage2_count: int):\n",
        "        \"\"\"Print detailed annotation statistics\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"TWO-STAGE SILVER LABEL GENERATION COMPLETE\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        print(f\"\\nðŸ“Š Pipeline Statistics:\")\n",
        "        print(f\"   Total processed: {len(df)}\")\n",
        "        print(f\"   Stage 1 only (Neutral): {stage1_count} ({stage1_count/len(df)*100:.1f}%)\")\n",
        "        print(f\"   Stage 2 API calls (Biased): {stage2_count} ({stage2_count/len(df)*100:.1f}%)\")\n",
        "        print(f\"   ðŸ’° Cost savings: {stage1_count/len(df)*100:.1f}% of sentences skipped Stage 2\")\n",
        "\n",
        "        error_count = len(df[df['Subtype_Silver_Label'] == 'ERROR'])\n",
        "        print(f\"\\n   Errors: {error_count} ({error_count/len(df)*100:.2f}%)\")\n",
        "\n",
        "        print(\"\\nðŸ“Š Bias Type Distribution:\")\n",
        "        dist = df['Subtype_Silver_Label'].value_counts()\n",
        "        for bias_type, count in dist.items():\n",
        "            pct = count / len(df) * 100\n",
        "            print(f\"   {bias_type:30s}: {count:4d} ({pct:5.2f}%)\")\n",
        "\n",
        "        valid_conf = df[df['Subtype_Silver_Label'] != 'ERROR']['Silver_Confidence']\n",
        "        if len(valid_conf) > 0:\n",
        "            print(f\"\\nðŸ“ˆ Confidence Statistics:\")\n",
        "            print(f\"   Mean:   {valid_conf.mean():.3f}\")\n",
        "            print(f\"   Median: {valid_conf.median():.3f}\")\n",
        "            print(f\"   Std:    {valid_conf.std():.3f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE I.3-I.5: MASTER DATASET & SPLITS\n",
        "# ============================================================================\n",
        "\n",
        "class DatasetBuilder:\n",
        "    \"\"\"Create master dataset and train/val/test splits\"\"\"\n",
        "\n",
        "    def __init__(self, config: ProjectConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def create_master_dataset(self, df_annotated: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create master dataset from annotated data\"\"\"\n",
        "        print(f\"\\n=== Phase I.4: Create Master Dataset ===\")\n",
        "\n",
        "        master_df = df_annotated.copy()\n",
        "\n",
        "        # Save master dataset\n",
        "        output_path = os.path.join(self.config.OUTPUT_DIR, 'master_dataset.csv')\n",
        "        master_df.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"âœ“ Master dataset: {len(master_df)} sentences\")\n",
        "        print(f\"  Saved: {output_path}\")\n",
        "\n",
        "        return master_df\n",
        "\n",
        "    def create_splits(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, ...]:\n",
        "        \"\"\"Create stratified train/val/test splits\"\"\"\n",
        "        print(f\"\\n=== Phase I.5: Data Split ===\")\n",
        "\n",
        "        # Remove errors\n",
        "        df_clean = df[df['Subtype_Silver_Label'] != 'ERROR'].copy()\n",
        "\n",
        "        # Check if we have enough data per class\n",
        "        class_counts = df_clean['Subtype_Silver_Label'].value_counts()\n",
        "        min_class_count = class_counts.min()\n",
        "\n",
        "        if min_class_count < 2:\n",
        "            print(f\"âš ï¸  Warning: Some classes have < 2 samples. Skipping stratification.\")\n",
        "            # Non-stratified split\n",
        "            train_df, temp_df = train_test_split(\n",
        "                df_clean,\n",
        "                test_size=(self.config.VAL_RATIO + self.config.TEST_RATIO),\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            val_ratio = self.config.VAL_RATIO / (\n",
        "                self.config.VAL_RATIO + self.config.TEST_RATIO\n",
        "            )\n",
        "            val_df, test_df = train_test_split(\n",
        "                temp_df,\n",
        "                test_size=(1 - val_ratio),\n",
        "                random_state=42\n",
        "            )\n",
        "        else:\n",
        "            # Stratified split\n",
        "            train_df, temp_df = train_test_split(\n",
        "                df_clean,\n",
        "                test_size=(self.config.VAL_RATIO + self.config.TEST_RATIO),\n",
        "                stratify=df_clean['Subtype_Silver_Label'],\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            val_ratio = self.config.VAL_RATIO / (\n",
        "                self.config.VAL_RATIO + self.config.TEST_RATIO\n",
        "            )\n",
        "            val_df, test_df = train_test_split(\n",
        "                temp_df,\n",
        "                test_size=(1 - val_ratio),\n",
        "                stratify=temp_df['Subtype_Silver_Label'],\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "        total = len(df_clean)\n",
        "        print(f\"\\nðŸ“Š Dataset Split:\")\n",
        "        print(f\"  Total:  {total:5d}\")\n",
        "        print(f\"  Train:  {len(train_df):5d} ({len(train_df)/total*100:5.1f}%)\")\n",
        "        print(f\"  Val:    {len(val_df):5d} ({len(val_df)/total*100:5.1f}%)\")\n",
        "        print(f\"  Test:   {len(test_df):5d} ({len(test_df)/total*100:5.1f}%)\")\n",
        "\n",
        "        # Save splits\n",
        "        for name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "            path = os.path.join(self.config.OUTPUT_DIR, f'{name}_split.csv')\n",
        "            split_df.to_csv(path, index=False)\n",
        "            print(f\"  âœ“ {name}_split.csv\")\n",
        "\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PHASE III: EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "class BiasEvaluator:\n",
        "    \"\"\"Comprehensive evaluation metrics\"\"\"\n",
        "\n",
        "    def __init__(self, config: ProjectConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def evaluate_multiclass(self, y_true: List[str], y_pred: List[str]) -> Dict:\n",
        "        \"\"\"Multi-class classification report\"\"\"\n",
        "        print(\"\\n=== Phase III: Multi-Class Evaluation ===\")\n",
        "\n",
        "        class_names = self.config.BIAS_SUBTYPES + ['None']\n",
        "\n",
        "        report = classification_report(\n",
        "            y_true, y_pred,\n",
        "            target_names=class_names,\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
        "\n",
        "        return report\n",
        "\n",
        "    def export_human_validation_sample(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Export sample for manual review\"\"\"\n",
        "        print(f\"\\n=== Human Validation Sample ===\")\n",
        "\n",
        "        n = min(self.config.HUMAN_VALIDATION_SAMPLE_SIZE, len(df))\n",
        "\n",
        "        sample_df = df.sample(n=n, random_state=42).copy()\n",
        "\n",
        "        sample_df['Judge_1'] = ''\n",
        "        sample_df['Judge_2'] = ''\n",
        "        sample_df['Judge_3'] = ''\n",
        "        sample_df['Consensus'] = ''\n",
        "        sample_df['Notes'] = ''\n",
        "\n",
        "        output_path = os.path.join(self.config.OUTPUT_DIR, 'human_validation.csv')\n",
        "        sample_df.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"âœ“ Sampled {len(sample_df)} sentences\")\n",
        "        print(f\"  Saved: {output_path}\")\n",
        "        print(\"\\nDistribution:\")\n",
        "        print("
      ]
    }
  ]
}