{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "COMPLETE NOTEBOOK 3: INFERENCE & EXPLAINABILITY (LIME)\n",
        "================================================================================\n",
        "This notebook:\n",
        "1. Loads the best trained model\n",
        "2. Provides inference functions for new text\n",
        "3. Generates LIME explanations\n",
        "4. Performs similarity search using FAISS\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sentence embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# FAISS\n",
        "import faiss\n",
        "\n",
        "# LIME for explainability\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MBIC BIAS DETECTION - COMPLETE PIPELINE\")\n",
        "print(\"Notebook 3: Inference & Explainability\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")"
      ],
      "metadata": {
        "id": "-UyxUFSymSRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 1: LOAD ALL ARTIFACTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: LOADING ARTIFACTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load model\n",
        "best_model = joblib.load('/content/best_model_final.pkl')\n",
        "print(\"âœ“ Loaded: best_model_final.pkl\")\n",
        "\n",
        "# Load metadata\n",
        "metadata = joblib.load('/content/model_metadata.pkl')\n",
        "print(\"âœ“ Loaded: model_metadata.pkl\")\n",
        "print(f\"  Model: {metadata['model_name']}\")\n",
        "print(f\"  Test F1 Macro: {metadata['test_f1_macro']:.4f}\")\n",
        "\n",
        "# Load label encoder\n",
        "label_encoder = joblib.load('/content/label_encoder.pkl')\n",
        "print(\"âœ“ Loaded: label_encoder.pkl\")\n",
        "\n",
        "# Load feature scaler\n",
        "feature_scaler = joblib.load('/content/feature_scaler.pkl')\n",
        "print(\"âœ“ Loaded: feature_scaler.pkl\")\n",
        "\n",
        "# Load linguistic extractor\n",
        "ling_extractor = joblib.load('/content/linguistic_extractor.pkl')\n",
        "print(\"âœ“ Loaded: linguistic_extractor.pkl\")\n",
        "\n",
        "# Load FAISS index\n",
        "faiss_index = faiss.read_index('/content/mbic_faiss_hybrid.index')\n",
        "print(f\"âœ“ Loaded: mbic_faiss_hybrid.index ({faiss_index.ntotal} vectors)\")\n",
        "\n",
        "# Load processed dataset (for similarity search)\n",
        "processed_df = pd.read_excel('/content/processed_dataset.xlsx')\n",
        "print(f\"âœ“ Loaded: processed_dataset.xlsx ({len(processed_df)} samples)\")\n",
        "\n",
        "# Load embedding models\n",
        "print(\"\\n Loading embedding models...\")\n",
        "model_mpnet = SentenceTransformer('all-mpnet-base-v2')\n",
        "model_minilm = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"âœ“ Embedding models loaded\")"
      ],
      "metadata": {
        "id": "EBi6CAHRme9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 2: CREATE INFERENCE PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: CREATING INFERENCE PIPELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class BiasDetectionPipeline:\n",
        "    \"\"\"Complete inference pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, model, label_encoder, model_mpnet, model_minilm,\n",
        "                 ling_extractor, feature_scaler):\n",
        "        self.model = model\n",
        "        self.label_encoder = label_encoder\n",
        "        self.model_mpnet = model_mpnet\n",
        "        self.model_minilm = model_minilm\n",
        "        self.ling_extractor = ling_extractor\n",
        "        self.feature_scaler = feature_scaler\n",
        "\n",
        "    def extract_features(self, text):\n",
        "        \"\"\"Extract hybrid features for a single text\"\"\"\n",
        "        # Embeddings\n",
        "        emb_mpnet = self.model_mpnet.encode([text])[0]\n",
        "        emb_minilm = self.model_minilm.encode([text])[0]\n",
        "        embeddings = np.concatenate([emb_mpnet, emb_minilm])\n",
        "\n",
        "        # Linguistic features\n",
        "        ling_features = self.ling_extractor.extract(text)\n",
        "        ling_array = np.array(list(ling_features.values())).reshape(1, -1)\n",
        "        ling_scaled = self.feature_scaler.transform(ling_array).flatten()\n",
        "\n",
        "        # Combine\n",
        "        features = np.concatenate([embeddings, ling_scaled])\n",
        "        return features\n",
        "\n",
        "    def predict(self, text):\n",
        "        \"\"\"Predict bias label for text\"\"\"\n",
        "        features = self.extract_features(text).reshape(1, -1)\n",
        "\n",
        "        # Handle different model types\n",
        "        prediction = self.model.predict(features)\n",
        "\n",
        "        # Convert to label if needed\n",
        "        if isinstance(prediction[0], (int, np.integer)):\n",
        "            label = self.label_encoder.inverse_transform(prediction)[0]\n",
        "        else:\n",
        "            label = prediction[0]\n",
        "\n",
        "        return label\n",
        "\n",
        "    def predict_proba(self, text):\n",
        "        \"\"\"Get prediction probabilities if available\"\"\"\n",
        "        features = self.extract_features(text).reshape(1, -1)\n",
        "\n",
        "        # Check if model has predict_proba\n",
        "        if hasattr(self.model, 'predict_proba'):\n",
        "            proba = self.model.predict_proba(features)[0]\n",
        "            return {label: prob for label, prob in zip(self.label_encoder.classes_, proba)}\n",
        "        elif hasattr(self.model, 'calibrated_classifier'):\n",
        "            # For confidence-based classifier\n",
        "            proba = self.model.calibrated_classifier.predict_proba(features)[0]\n",
        "            # Map binary proba to three classes\n",
        "            return {\n",
        "                'Biased': proba[1],\n",
        "                'Non-biased': proba[0],\n",
        "                'No agreement': 1.0 - proba.max()\n",
        "            }\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def predict_batch(self, texts):\n",
        "        \"\"\"Predict for multiple texts\"\"\"\n",
        "        return [self.predict(text) for text in texts]\n",
        "\n",
        "# Create pipeline instance\n",
        "pipeline = BiasDetectionPipeline(\n",
        "    model=best_model,\n",
        "    label_encoder=label_encoder,\n",
        "    model_mpnet=model_mpnet,\n",
        "    model_minilm=model_minilm,\n",
        "    ling_extractor=ling_extractor,\n",
        "    feature_scaler=feature_scaler\n",
        ")\n",
        "\n",
        "print(\"âœ“ Inference pipeline created\")"
      ],
      "metadata": {
        "id": "TPT0sf1MmjG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 3: TEST INFERENCE ON EXAMPLES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: TESTING INFERENCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test examples\n",
        "test_examples = [\n",
        "    \"The politician was caught red-handed in a massive corruption scandal.\",\n",
        "    \"The study was conducted using standard scientific methodology.\",\n",
        "    \"Some people believe the earth is flat, but this claim has been debunked.\",\n",
        "    \"All immigrants are criminals and should be deported immediately.\",\n",
        "    \"The data shows a correlation between the two variables.\",\n",
        "]\n",
        "\n",
        "print(\"\\nTest predictions:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, text in enumerate(test_examples, 1):\n",
        "    prediction = pipeline.predict(text)\n",
        "    proba = pipeline.predict_proba(text)\n",
        "\n",
        "    print(f\"\\n{i}. Text: {text}\")\n",
        "    print(f\"   Prediction: {prediction}\")\n",
        "    if proba:\n",
        "        print(f\"   Probabilities:\")\n",
        "        for label, prob in sorted(proba.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"     {label:15s}: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "lA8BFZeWmsEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 4: SIMILARITY SEARCH WITH FAISS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: SIMILARITY SEARCH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def find_similar_samples(query_text, k=5):\n",
        "    \"\"\"Find k most similar samples in the dataset\"\"\"\n",
        "    # Extract features for query\n",
        "    query_features = pipeline.extract_features(query_text).reshape(1, -1)\n",
        "\n",
        "    # Search FAISS index\n",
        "    distances, indices = faiss_index.search(query_features.astype('float32'), k)\n",
        "\n",
        "    results = []\n",
        "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        results.append({\n",
        "            'rank': i + 1,\n",
        "            'index': idx,\n",
        "            'distance': dist,\n",
        "            'similarity': 1 / (1 + dist),  # Convert distance to similarity\n",
        "            'text': processed_df.iloc[idx]['sentence'],\n",
        "            'label': processed_df.iloc[idx]['Label_bias']\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Test similarity search\n",
        "print(\"\\nTesting similarity search...\")\n",
        "query = \"The corrupt politician stole millions from taxpayers.\"\n",
        "print(f\"\\nQuery: {query}\")\n",
        "print(f\"Prediction: {pipeline.predict(query)}\\n\")\n",
        "\n",
        "similar = find_similar_samples(query, k=5)\n",
        "print(\"Top 5 similar samples:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for result in similar:\n",
        "    print(f\"\\n{result['rank']}. Similarity: {result['similarity']:.4f}\")\n",
        "    print(f\"   Label: {result['label']}\")\n",
        "    print(f\"   Text: {result['text'][:100]}...\")"
      ],
      "metadata": {
        "id": "jV5JYJ-cm3B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 5: LIME EXPLAINABILITY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: LIME EXPLAINABILITY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create LIME explainer\n",
        "class_names = list(label_encoder.classes_)\n",
        "\n",
        "# Wrapper function for LIME (must return probabilities)\n",
        "def predict_fn_for_lime(texts):\n",
        "    \"\"\"Wrapper for LIME that returns probability matrix\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for text in texts:\n",
        "        proba = pipeline.predict_proba(text)\n",
        "\n",
        "        if proba:\n",
        "            # Convert dict to array in correct order\n",
        "            proba_array = [proba.get(label, 0.0) for label in class_names]\n",
        "        else:\n",
        "            # If no probabilities, create one-hot encoding\n",
        "            pred = pipeline.predict(text)\n",
        "            proba_array = [1.0 if label == pred else 0.0 for label in class_names]\n",
        "\n",
        "        results.append(proba_array)\n",
        "\n",
        "    return np.array(results)\n",
        "\n",
        "# Create explainer\n",
        "print(\"\\nInitializing LIME explainer...\")\n",
        "explainer = LimeTextExplainer(\n",
        "    class_names=class_names,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "print(\"âœ“ LIME explainer ready\")\n",
        "\n",
        "def explain_prediction(text, num_features=10):\n",
        "    \"\"\"Generate LIME explanation for a prediction\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"LIME EXPLANATION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nText: {text}\")\n",
        "\n",
        "    # Get prediction\n",
        "    prediction = pipeline.predict(text)\n",
        "    proba = pipeline.predict_proba(text)\n",
        "\n",
        "    print(f\"\\nPrediction: {prediction}\")\n",
        "    if proba:\n",
        "        print(\"Probabilities:\")\n",
        "        for label, prob in sorted(proba.items(), key=lambda x: x[1], reverse=True):\n",
        "            print(f\"  {label:15s}: {prob:.4f}\")\n",
        "\n",
        "    # Generate explanation\n",
        "    print(f\"\\nGenerating LIME explanation...\")\n",
        "    exp = explainer.explain_instance(\n",
        "        text,\n",
        "        predict_fn_for_lime,\n",
        "        num_features=num_features,\n",
        "        num_samples=500\n",
        "    )\n",
        "\n",
        "    # Show explanation\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"FEATURE IMPORTANCE\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Get prediction class index\n",
        "    pred_idx = list(class_names).index(prediction) if prediction in class_names else 0\n",
        "\n",
        "    # Show top features\n",
        "    print(f\"\\nTop {num_features} features for prediction '{prediction}':\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Feature':<30} {'Weight':<15} {'Impact'}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for feature, weight in exp.as_list(label=pred_idx):\n",
        "        impact = \"Supports âœ“\" if weight > 0 else \"Against âœ—\"\n",
        "        print(f\"{feature:<30} {weight:>+.4f}        {impact}\")\n",
        "\n",
        "    return exp"
      ],
      "metadata": {
        "id": "ABwh995enI9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test LIME on examples\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING EXPLANATIONS FOR TEST EXAMPLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Example 1: Clearly biased\n",
        "exp1 = explain_prediction(\n",
        "    \"All politicians are corrupt liars who only care about themselves.\",\n",
        "    num_features=8\n",
        ")\n",
        "\n",
        "# Example 2: Neutral\n",
        "exp2 = explain_prediction(\n",
        "    \"The committee met yesterday to discuss the proposed legislation.\",\n",
        "    num_features=8\n",
        ")\n",
        "\n",
        "# Example 3: Ambiguous\n",
        "exp3 = explain_prediction(\n",
        "    \"Some experts believe this policy might have unintended consequences.\",\n",
        "    num_features=8\n",
        ")"
      ],
      "metadata": {
        "id": "cAjNaZKDnLJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# STEP 6: CREATE INTERACTIVE INFERENCE FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 6: INTERACTIVE INFERENCE FUNCTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def analyze_text_complete(text, show_similar=True, show_explanation=True):\n",
        "    \"\"\"\n",
        "    Complete analysis of a text:\n",
        "    - Prediction with probabilities\n",
        "    - Similar samples from dataset\n",
        "    - LIME explanation\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPLETE BIAS ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nInput: {text}\")\n",
        "\n",
        "    # Prediction\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(\"1. PREDICTION\")\n",
        "    print(\"-\"*80)\n",
        "    prediction = pipeline.predict(text)\n",
        "    proba = pipeline.predict_proba(text)\n",
        "\n",
        "    print(f\"\\nPredicted Label: {prediction}\")\n",
        "    if proba:\n",
        "        print(\"\\nConfidence Scores:\")\n",
        "        for label, prob in sorted(proba.items(), key=lambda x: x[1], reverse=True):\n",
        "            bar = \"â–ˆ\" * int(prob * 40)\n",
        "            print(f\"  {label:15s}: {prob:.4f} {bar}\")\n",
        "\n",
        "    # Similar samples\n",
        "    if show_similar:\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"2. SIMILAR SAMPLES FROM DATASET\")\n",
        "        print(\"-\"*80)\n",
        "        similar = find_similar_samples(text, k=3)\n",
        "        for result in similar:\n",
        "            print(f\"\\n  {result['rank']}. Similarity: {result['similarity']:.4f} | Label: {result['label']}\")\n",
        "            print(f\"     {result['text'][:80]}...\")\n",
        "\n",
        "    # LIME explanation\n",
        "    if show_explanation:\n",
        "        print(\"\\n\" + \"-\"*80)\n",
        "        print(\"3. FEATURE IMPORTANCE (LIME)\")\n",
        "        print(\"-\"*80)\n",
        "        exp = explainer.explain_instance(\n",
        "            text,\n",
        "            predict_fn_for_lime,\n",
        "            num_features=6,\n",
        "            num_samples=300\n",
        "        )\n",
        "\n",
        "        pred_idx = list(class_names).index(prediction) if prediction in class_names else 0\n",
        "        print(f\"\\nKey words influencing '{prediction}' prediction:\")\n",
        "        for feature, weight in exp.as_list(label=pred_idx)[:6]:\n",
        "            impact = \"+\" if weight > 0 else \"-\"\n",
        "            print(f\"  {impact} {feature:<25} (weight: {weight:>+.4f})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Example usage\n",
        "print(\"\\nâœ“ Interactive function ready. Example usage:\")\n",
        "print(\"\\n  analyze_text_complete('Your text here')\")\n",
        "\n",
        "# Test it\n",
        "analyze_text_complete(\n",
        "    \"This radical extremist policy will destroy our economy and lead to chaos.\",\n",
        "    show_similar=True,\n",
        "    show_explanation=True\n",
        ")"
      ],
      "metadata": {
        "id": "paoLSyLDnU3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# STEP 7: BATCH INFERENCE FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 7: BATCH INFERENCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def predict_batch_with_details(texts):\n",
        "    \"\"\"Predict multiple texts and return detailed results\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        prediction = pipeline.predict(text)\n",
        "        proba = pipeline.predict_proba(text)\n",
        "\n",
        "        result = {\n",
        "            'index': i,\n",
        "            'text': text,\n",
        "            'prediction': prediction,\n",
        "        }\n",
        "\n",
        "        if proba:\n",
        "            result.update({\n",
        "                'prob_biased': proba.get('Biased', 0.0),\n",
        "                'prob_non_biased': proba.get('Non-biased', 0.0),\n",
        "                'prob_no_agreement': proba.get('No agreement', 0.0),\n",
        "                'confidence': max(proba.values())\n",
        "            })\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Test batch inference\n",
        "batch_texts = [\n",
        "    \"The research methodology followed established protocols.\",\n",
        "    \"These corrupt officials are destroying our country!\",\n",
        "    \"The data suggests a possible correlation.\",\n",
        "    \"All members of that group are dangerous criminals.\",\n",
        "    \"The committee will review the proposal next week.\"\n",
        "]\n",
        "\n",
        "print(\"\\nBatch inference example:\")\n",
        "results_df = predict_batch_with_details(batch_texts)\n",
        "print(\"\\n\", results_df[['text', 'prediction', 'confidence']].to_string(index=False))"
      ],
      "metadata": {
        "id": "C8qdzNHynYq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4caH76EOkK0u"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"NOTEBOOK 3 COMPLETE - SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nâœ… INFERENCE PIPELINE READY\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Available Functions:\")\n",
        "print(f\"  1. pipeline.predict(text) - Single prediction\")\n",
        "print(f\"  2. pipeline.predict_proba(text) - With probabilities\")\n",
        "print(f\"  3. pipeline.predict_batch(texts) - Batch prediction\")\n",
        "print(f\"  4. find_similar_samples(text, k=5) - Find similar samples\")\n",
        "print(f\"  5. explain_prediction(text) - LIME explanation\")\n",
        "print(f\"  6. analyze_text_complete(text) - Complete analysis\")\n",
        "print(f\"  7. predict_batch_with_details(texts) - Batch with details\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ USAGE EXAMPLES:\")\n",
        "print(f\"\"\"\n",
        "# Single prediction\n",
        "pred = pipeline.predict(\"Your text here\")\n",
        "\n",
        "# With probabilities\n",
        "proba = pipeline.predict_proba(\"Your text here\")\n",
        "\n",
        "# Complete analysis\n",
        "analyze_text_complete(\"Your text here\")\n",
        "\n",
        "# Batch processing\n",
        "df = pd.DataFrame({{'text': your_texts}})\n",
        "df['prediction'] = pipeline.predict_batch(df['text'].tolist())\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ MODEL PERFORMANCE:\")\n",
        "print(f\"  Model: {metadata['model_name']}\")\n",
        "print(f\"  Test F1 Macro: {metadata['test_f1_macro']:.4f}\")\n",
        "print(f\"  Test Accuracy: {metadata['test_accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL NOTEBOOKS COMPLETE - SYSTEM READY FOR DEPLOYMENT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nCompleted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)"
      ]
    }
  ]
}